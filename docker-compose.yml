version: '3.7'

services:
#    llama-cpp-python:
#        image: 3x3cut0r/llama-cpp-python:latest
#        container_name: llama-cpp-python-jarvis
#        cap_add:
#            - SYS_RESOURCE
#        environment:
#            MODEL_DOWNLOAD: "True" 
#            MODEL_REPO: "cfahlgren1/natural-functions-GGUF" 
#            MODEL: "natural-functions.Q6_K.gguf" 
#            MODEL_ALIAS: "natural-functions" 
#            DEFAULT_MODEL: "natural-functions.Q6_K.gguf" 
#            N_GPU_LAYERS : 32
#            N_CTX : 4096
#        volumes:
#            - ./model:/model
#        ports:
#            - '8000:8000'

    ollama:
        image: ollama/ollama:0.1.29
        container_name: ollama
        ports:
        - 8085:11434
        volumes:
        - ./models:/root/.ollama/models
        environment:
            OLLAMA_ORIGINS: '*' 
            #OLLAMA_HOST: "localhost:11434"
            #OLLAMA_PORT: 11434  
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          #device_ids: ['0']
                          count: 1
                          capabilities: [gpu]
        #command: "ollama pull calebfahlgren/natural-functions"                  
        command: 
            - "serve"

    mariadb:
        image: mariadb:10.6.4-focal
        volumes:
            - type: volume
              source: dbdata
              target: /var/lib/mysql
        environment:
            TZ: 'Europe/Rome'
            MYSQL_ALLOW_EMPTY_PASSWORD: 'no'
            MYSQL_ROOT_PASSWORD: '${MYSQL_ROOT_PASSWORD}'
            MYSQL_USER: '${MYSQL_USER}'
            MYSQL_PASSWORD: '${MYSQL_PASSWORD}'
            MYSQL_DATABASE: '${MYSQL_DATABASE}'
    adminer:
        image: adminer
        restart: always
        ports:
        - 81:8080

    php-apache:
        build:
            context: .
            dockerfile: ./docker/php-apache/Dockerfile
        environment:
            APIKEY: ${APIKEY}
            MODEL: ${MODEL}
            MAIL_HOST: ${MAIL_HOST}
            MAIL_PORT: ${MAIL_PORT}
            MAIL_FROM: ${MAIL_FROM}
            MAIL_USERNAME: ${MAIL_USERNAME}
            MAIL_PASSWORD: ${MAIL_PASSWORD}
            PORTAINER_URL: ${PORTAINER_URL}
            PORTAINER_USERNAME: ${PORTAINER_USERNAME}
            PORTAINER_PASSWORD: ${PORTAINER_PASSWORD}
        volumes:
            #- type: bind
            #  source: ai_content
            #  target: /usr/src/app/ai_content
            #- type: volume
            #  source: uploads
            #  target: /usr/src/app/public/uploads
            - ./src/app:/usr/src/app

        ports:
            - '80:80'


    langchain:
        build:
            context: ./docker/langchain
            dockerfile: Dockerfile
        command: tail -f /dev/null
        ports:
        - "8000:8000"
        environment:
        - DATA_PATH=${DATA_PATH}
        - OPENAI_API_KEY=${OPENAI_API_KEY}
        deploy:
            resources:
                reservations:
                    devices:
                    - driver: nvidia
                      device_ids: ['0']
                      capabilities: [gpu]


    weaviate:
        command:
        - --host
        - 0.0.0.0
        - --port
        - '8080'
        - --scheme
        - http
        image: cr.weaviate.io/semitechnologies/weaviate:latest
        ports:
        - 8080:8080
        - 50051:50051
        volumes:
        - weaviate_data:/var/lib/weaviate
        restart: on-failure:0
        environment:
            QUERY_DEFAULTS_LIMIT: 25
            AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
            PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
            DEFAULT_VECTORIZER_MODULE: 'none'
            ENABLE_MODULES: 'text2vec-cohere,text2vec-huggingface,text2vec-palm,text2vec-openai,generative-openai,generative-cohere,generative-palm,ref2vec-centroid,reranker-cohere,qna-openai'
            CLUSTER_HOSTNAME: 'node1'
volumes:
    weaviate_data:
#    ai_content:
#    uploads:
    dbdata:
    ollama:

